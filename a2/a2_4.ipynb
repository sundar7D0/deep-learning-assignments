{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"a2_4.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyNQGn9PG/A5NOUzGhWnQGII"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"gL0WURoDOz-B","colab_type":"code","colab":{}},"source":["#-1: Setting up google colab:\n","from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)\n","%cd '/content/gdrive/My Drive/CS6910_DL/a2/'\n","!ls\n","#!unzip data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dPD2byowO_G7","colab_type":"code","colab":{}},"source":["#0. Importing modules & defining global variables:\n","import os\n","import random\n","import numpy as np \n","import pandas as pd \n","import tensorflow as tf \n","from tensorflow import keras \n","from matplotlib import pyplot as plot\n","print(tf.__version__) \n","\n","#categories=['Ankle boot']\n","categories=['Ankle boot','Bag','Coat','Sandal','Sneaker']\n","dataset,labels=[],[]\n","dir='./data/data2/'\n","activation_fn,batchsize,max_patience_pt,max_patience_ft,max_epochs_pt,max_epochs_ft = 'sigmoid',20,20,20,10,100\n","d1,d2,d3=300,150,70\n","d4,d5=30,len(categories)\n","K,learning_rate,momentum=2,0.01,0.95"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZiNN91t3PYWG","colab_type":"code","colab":{}},"source":["#1. Image pre-processing & data-loading function:\n","def data():\n","    dataset,labels=[],[]\n","    for i in range(len(categories)):\n","        filename=categories[i]+'.csv'\n","        print('Reading ',filename,'...')\n","        temp=pd.read_csv(dir+filename,header=None)\n","        min_temp=temp.T.describe().T['min']\n","        max_temp=temp.T.describe().T['max']\n","        temp=((temp.T-min_temp)/(max_temp-min_temp)).T.to_numpy()\n","        dataset.append(temp)\n","        labels.append(np.repeat(i,temp.shape[0]))\n","    dataset=np.array(np.concatenate(dataset,axis=0),dtype=np.float32)\n","    labels=np.concatenate(labels,axis=0)\n","    combined=list(zip(dataset,labels))\n","    random.shuffle(combined)\n","    dataset,labels=zip(*combined)\n","    dataset,labels=np.array(dataset),np.array(labels)\n","    print('Shape of dataset & labels: ',dataset.shape,' & ',labels.shape)\n","    return dataset,labels\n","dataset,labels=data()\n","train_dataset,validate_dataset,test_dataset=np.split(dataset,[int(.7*len(dataset)),int(.8*len(dataset))])\n","train_labels,validate_labels,test_labels=np.split(labels,[int(.7*len(labels)),int(.8*len(labels))])\n","print('Shape of dataset & labels (train, valid, test): (',train_dataset.shape,',',validate_dataset.shape,',',test_dataset.shape,')',' & (',train_labels.shape,',',validate_labels.shape,',',test_labels.shape,')')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2IDiR3CG9l4N","colab_type":"code","colab":{}},"source":["#2. Classes & functions for building model:\n","\n","class Model(object):\n","\tdef __init__(self,input_shape,n=3):\n","\t\tself.n=n\n","\t\tself.w01 = tf.Variable(tf.initializers.GlorotUniform()(shape=(d1,input_shape)),dtype=tf.float32)\n","\t\tself.b0 = tf.Variable(tf.initializers.Zeros()(shape=input_shape),dtype=tf.float32)\n","\t\tself.x00 = tf.Variable(tf.initializers.Zeros()(shape=input_shape),dtype=tf.float32)\n","\t\tself.x0k = tf.Variable(tf.initializers.Zeros()(shape=input_shape),dtype=tf.float32)\n","\t\tself.b1 = tf.Variable(tf.initializers.Zeros()(shape=d1),dtype=tf.float32)\n","\t\tself.x10 = tf.Variable(tf.initializers.Zeros()(shape=d1),dtype=tf.float32)\n","\t\tself.x1k = tf.Variable(tf.initializers.Zeros()(shape=d1),dtype=tf.float32)\n","\t\tif n >= 2:\n","\t\t\tself.w12 = tf.Variable(tf.initializers.GlorotUniform()(shape=(d2,d1)),dtype=tf.float32)\n","\t\t\tself.b2 = tf.Variable(tf.initializers.Zeros()(shape=d2),dtype=tf.float32)\n","\t\t\tself.x20 = tf.Variable(tf.initializers.Zeros()(shape=d2),dtype=tf.float32)\n","\t\t\tself.x2k = tf.Variable(tf.initializers.Zeros()(shape=d2),dtype=tf.float32)\n","\t\tif n >= 3:\n","\t\t\tself.w23 = tf.Variable(tf.initializers.GlorotUniform()(shape=(d3,d2)),dtype=tf.float32)\n","\t\t\tself.b3 = tf.Variable(tf.initializers.Zeros()(shape=d3),dtype=tf.float32)\n","\t\t\tself.x30 = tf.Variable(tf.initializers.Zeros()(shape=d3),dtype=tf.float32)\n","\t\t\tself.x3k = tf.Variable(tf.initializers.Zeros()(shape=d3),dtype=tf.float32)\n","#\t\tself.sigma = tf.Variable(tf.initializers.Ones()(shape=input_shape),dtype=tf.float32)\n","\n","\tdef __call__(self,x0):\n","\t\tif self.n is 3:\t\t\n","\t\t\treturn self.bins(tf.linalg.matvec(self.w23,self.bins(tf.linalg.matvec(self.w12,self.bins(tf.linalg.matvec(self.w01,x0)+self.b1))+self.b2))+self.b3)\n","\t\telif self.n is 2:\n","\t\t\treturn self.bins(tf.linalg.matvec(self.w12,self.bins(tf.linalg.matvec(self.w01,x0)+self.b1))+self.b2)\n","\t\telif self.n is 1:\n","\t\t\treturn self.bins(tf.linalg.matvec(self.w01,x0)+self.b1)\n","\n","\tdef feedfrwd(self,x0,id=0):\n","\t\tif id is 1:\n","\t\t\tself.x00=x0\n","\t\t\tself.x10=tf.math.sigmoid(tf.linalg.matvec(self.w01,self.x00)+self.b1)\n","\t\t\tout_b1=self.bins(self.x10,1)\n","\t\t\tif self.n is 1:\n","\t\t\t\treturn out_b1\n","\t\t\tself.x20=tf.math.sigmoid(tf.linalg.matvec(self.w12,out_b1)+self.b2)\n","\t\t\tout_b2=self.bins(self.x20,1)\n","\t\t\tif self.n is 2:\n","\t\t\t\treturn out_b2\n","\t\t\tself.x30=tf.math.sigmoid(tf.linalg.matvec(self.w23,out_b2)+self.b3)\n","\t\t\tout_b3=self.bins(self.x30,1)\n","\t\t\treturn out_b3\n","\t\telse:\n","\t\t\tif self.n is 1:\n","\t\t\t\treturn self.bins(tf.linalg.matvec(self.w01,x0)+self.b1)\n","\t\t\telif self.n is 2:\n","\t\t\t\treturn self.bins(tf.linalg.matvec(self.w12,self.bins(tf.linalg.matvec(self.w01,x0)+self.b1))+self.b2)\n","\t\t\telse:\n","\t\t\t\treturn self.bins(tf.linalg.matvec(self.w23,self.bins(tf.linalg.matvec(self.w12,self.bins(tf.linalg.matvec(self.w01,x0)+self.b1))+self.b2))+self.b3)\n","\n","\tdef feedbkwd(self,xl,id=0):\n","\t\tif id is 1:\n","\t\t\tif self.n is 3:\n","\t\t\t\tself.x3k=self.bins(xl,1)\n","\t\t\t\tself.x2k=tf.math.sigmoid(tf.linalg.matvec(tf.transpose(self.w23),self.x3k)+self.b2)\n","\t\t\t\tout_b2=self.bins(self.x2k,1)\n","\t\t\t\tself.x1k=tf.math.sigmoid(tf.linalg.matvec(tf.transpose(self.w12),out_b2)+self.b1)\n","\t\t\t\tout_b1=self.bins(self.x1k)\n","\t\t\t\tself.x0k=self.bins(tf.linalg.matvec(tf.transpose(self.w01),out_b1)+self.b0)\n","\t\t\telif self.n is 2:\n","\t\t\t\tself.x2k=self.bins(xl,1)\n","\t\t\t\tself.x1k=tf.math.sigmoid(tf.linalg.matvec(tf.transpose(self.w12),self.x2k)+self.b1)\n","\t\t\t\tout_b1=self.bins(self.x1k)\n","\t\t\t\tself.x0k=self.bins(tf.linalg.matvec(tf.transpose(self.w01),out_b1)+self.b0)\n","\t\t\telif self.n is 1:\n","\t\t\t\tself.x1k=self.bins(xl,1)\n","\t\t\t\tself.x0k=self.bins(tf.linalg.matvec(tf.transpose(self.w01),self.x1k)+self.b0)\n","\t\t\treturn self.x0k\n","\t\telse:\n","\t\t\tif self.n is 3:\n","\t\t\t\treturn self.bins(tf.linalg.matvec(tf.transpose(self.w01),self.bins(tf.linalg.matvec(tf.transpose(self.w12),self.bins(tf.linalg.matvec(tf.transpose(self.w23),self.bins(xl,1))+self.b2))+self.b1))+self.b0)\n","\t\t\telif self.n is 2:\n","\t\t\t\treturn self.bins(tf.linalg.matvec(tf.transpose(self.w01),self.bins(tf.linalg.matvec(tf.transpose(self.w12),self.bins(xl,1))+self.b1))+self.b0)\n","\t\t\telif self.n is 1:\n","\t\t\t\treturn self.bins(tf.linalg.matvec(tf.transpose(self.w01),xl)+self.b0)\n","\n","\tdef bins(self,inp,id=0):\n","\t\tif id is 0:\n","\t\t\treturn tf.floor(tf.math.sigmoid(inp)+tf.random.uniform(tf.shape(inp),0,1))  #Sigmoid of input & then binarying.\n","#\t\t\treturn tf.floor(tf.math.sigmoid(inp)+0.5*tf.ones(shape=inp.shape))\n","\t\telse:\n","\t\t\treturn tf.floor(inp+tf.random.uniform(tf.shape(inp),0,1))  #Binarying.\n","#\t\t\treturn tf.floor(inp+tf.repeat(0.5,inp.shape))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTjDl8CzTycB","colab_type":"code","colab":{}},"source":["#3. Pre-training Restricted Boltzmann Machines:\n","\n","def pt_rbm(dataset,K=K,n=3):\n","    print(\"Pre-training RBM's...\")\n","    input_shape=dataset.shape[-1]\n","    model = Model(input_shape,n)\n","    min_valid_loss,patience=10,0\n","    inp,out=tf.Variable(tf.initializers.Zeros()(shape=input_shape),dtype=tf.float32),tf.Variable(tf.initializers.Zeros()(shape=d3),dtype=tf.float32)\n","    for epoch in range(max_epochs_pt+1):\n","        if epoch is 0:\n","            min_valid_loss=np.square(model.feedbkwd(model(validate_dataset))-validate_dataset).mean(axis=1).mean()\n","        validate_loss=np.square(model.feedbkwd(model(validate_dataset))-validate_dataset).mean(axis=1).mean()\n","        if min_valid_loss < validate_loss:\n","            min_valid_loss=validate_loss\n","            patience=0\n","        else:\n","            patience=patience+1\n","        if patience >= max_patience_pt:\n","            break\n","        for i in range(int(len(dataset))):\n","            inp=dataset[i]\n","            for k in range(K):\n","                print('In ',k+1,'th step of ',epoch,'/',max_epochs_pt,' epochs in ',i,'th dataset with n=',n)\n","                if k is 0:\n","                    out=model.feedfrwd(inp,1)\n","                    inp=model.feedbkwd(out,1)\n","                elif k is K-1:\n","                    out=model.feedfrwd(inp)\n","                    inp=model.feedbkwd(out,1)\n","                else:\n","                    out=model.feedfrwd(inp)\n","                    inp=model.feedbkwd(out)\n","            dw01 = tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w01,model.x00)+model.b1),model.x00,0) - tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w01,model.x0k)+model.b1),model.x0k,0)\n","            db0 = model.x00 - model.x0k\n","            db1_0 = tf.math.sigmoid(tf.linalg.matvec(model.w01,model.x00)+model.b1) - tf.math.sigmoid(tf.linalg.matvec(model.w01,model.x0k)+model.b1)\n","            db1 = db1_0\n","            if n>=2:\n","                db1_2 = model.x10 - model.x1k\n","                db1 = db1 + db1_2\n","                dw12 = tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w12,model.x10)+model.b2),model.x10,0) - tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w12,model.x1k)+model.b2),model.x1k,0)\n","                db2_1 = tf.math.sigmoid(tf.linalg.matvec(model.w12,model.x10)+model.b2) - tf.math.sigmoid(tf.linalg.matvec(model.w12,model.x1k)+model.b2)\n","                db2 = db2_1 \n","            if n>=3:\n","                db2_3 = model.x20 - model.x2k\n","                db2= db2 + db2_3\n","                dw23 = tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w23,model.x20)+model.b3),model.x20,0) - tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w23,model.x2k)+model.b3),model.x2k,0)\n","                db3 = tf.math.sigmoid(tf.linalg.matvec(model.w23,model.x20)+model.b3) - tf.math.sigmoid(tf.linalg.matvec(model.w23,model.x2k)+model.b3)\n","#            model.w01=model.w01*momentum + (learning_rate*(1-momentum)/tf.cast(tf.shape(dw01)[1],dtype=tf.float32))*dw01\t\t\n","#            model.b0=model.b0*momentum + (learning_rate*(1-momentum)/tf.cast(tf.shape(db0)[0],dtype=tf.float32))*db0\n","            model.w01=model.w01 + learning_rate*dw01\t\t\n","            model.b0=model.b0 + learning_rate*db0\n","            model.b1=model.b1 + learning_rate*db1\n","            if n >= 2:\n","                model.w12=model.w12 + learning_rate*dw12\t\t\n","                model.b2=model.b2 + learning_rate*db2\n","            if n >= 3:\n","                model.w23=model.w23 + learning_rate*dw23\t\t\n","                model.b3=model.b3 + learning_rate*db3          \n","    return model\n","\n","#model1=pt_rbm(dataset,K,1)\n","#model2=pt_rbm(dataset,K,2)\n","model3=pt_rbm(dataset,K,3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJjlPmgFT5fq","colab_type":"code","colab":{}},"source":["#4. Fine-tuning:\n","\n","def classifier(inp):\n","\tdense4=tf.keras.layers.Dense(d4,activation=activation_fn)\n","\tdense5=tf.keras.layers.Dense(d5,activation='softmax')\n","\treturn dense5(dense4(inp))\n","\n","def ft_dense():\n","    print('Fine tuning...')\n","    inp1=keras.Input(shape=d3)\n","    out1=classifier(inp1)\n","    model=keras.models.Model(inputs=inp1,outputs=out1)\n","    model.compile(optimizer='adam',loss=keras.losses.CategoricalCrossentropy(),metrics=['mae','mse','accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YJDoFNDWDXNe","colab_type":"code","colab":{}},"source":["#5. Functions for plotting model history & saving:\n","def plots(history):\n","    print(history.history.keys())\n","    plot.plot(history.history['accuracy'])\n","    plot.plot(history.history['val_accuracy'])\n","    plot.title('Model_Accuracy vs #Epochs')\n","    plot.ylabel('Accuracy')\n","    plot.xlabel('#Epochs')\n","    plot.legend(['Train', 'Validation'], loc='upper right')\n","    plot.grid()\n","    plot.show()\n","    plot.plot(history.history['loss'])\n","    plot.plot(history.history['val_loss'])\n","    plot.title('Model_Loss (CrossEntropy) vs #Epochs')\n","    plot.ylabel('Loss')\n","    plot.xlabel('#Epochs')\n","    plot.legend(['Train', 'Validation'], loc='upper right')\n","    plot.grid()\n","    plot.show()\n","\n","def save():\n","    inp1=keras.Input(shape=train_dataset.shape[-1])\n","    den1=keras.layers.Dense(d1,activation='sigmoid')\n","    den2=keras.layers.Dense(d2,activation='sigmoid')\n","    den3=keras.layers.Dense(d3,activation='sigmoid')\n","    den4=keras.layers.Dense(d4,activation=activation_fn)\n","    den5=keras.layers.Dense(d5,activation='softmax')\n","    out1=den5(den4(den3(den2(den1(inp1)))))\n","    den1.set_weights([tf.transpose(model3.w01),model3.b1])\n","    den2.set_weights([tf.transpose(model3.w12),model3.b2])\n","    den3.set_weights([tf.transpose(model3.w23),model3.b3])\n","    den4.set_weights(m3.get_weights()[0:2])\n","    den5.set_weights(m3.get_weights()[2:])\n","    save_model=keras.models.Model(inputs=inp1,outputs=out1)\n","    save_model.compile(optimizer='adam',loss=keras.losses.CategoricalCrossentropy(),metrics=['mae','mse','accuracy'])\n","    save_model.save('./models/a2_4/full_model')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jvD_yMOT6j9","colab_type":"code","colab":{}},"source":["#6. Integration:\n","if __name__ == '__main__':\n","#    m1=ft_dense()\n","#    m2=ft_dense()\n","    m3=ft_dense()\n","    earlystop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=max_patience)\n","#    history1=m1.fit(model1(train_dataset),tf.one_hot(train_labels,len(categories)),batch_size=batchsize,callbacks=[earlystop],epochs=max_epochs_ft,validation_data=(model1(validate_dataset),tf.one_hot(validate_labels,len(categories))))\n","#    history2=m2.fit(model2(train_dataset),tf.one_hot(train_labels,len(categories)),batch_size=batchsize,callbacks=[earlystop],epochs=max_epochs_ft,validation_data=(model2(validate_dataset),tf.one_hot(validate_labels,len(categories))))\n","    history3=m3.fit(model3(train_dataset),tf.one_hot(train_labels,len(categories)),batch_size=batchsize,callbacks=[earlystop],epochs=max_epochs_ft,validation_data=(model3(validate_dataset),tf.one_hot(validate_labels,len(categories))))\n","#1. Plotting history of model training:\n","    plots(history3)\n","#2. Results on test_data:\n","    result=m3.evaluate(model3(test_dataset),tf.one_hot(test_labels,len(categories)))\n","#3. Confusion Matrix:\n","    y_preds=m3.predict(model3(test_dataset))\n","    y_test=test_labels\n","    y_preds=np.argmax(y_preds, axis=1)\n","    conf_mat = metrics.confusion_matrix(y_test, y_preds)\n","    team_csvs=['Ankle boot','Bag','Coat','Sandal','Sneaker']\n","    sns.heatmap(conf_mat, annot=True, cmap='BuPu',xticklabels=team_csvs, yticklabels=team_csvs).set_title(\"Confusion Matrix\")\n","#4. Saving the model:\n","    save()\n"],"execution_count":null,"outputs":[]}]}