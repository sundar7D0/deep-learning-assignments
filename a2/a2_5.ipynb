{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"a2_5.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPUcxq16z0qmEgTGClccWbZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eBs3_AM8XE_c","colab_type":"code","colab":{}},"source":["#-1: Setting up google colab:\n","from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)\n","%cd '/content/gdrive/My Drive/CS6910_DL/a2/'\n","!ls\n","#!unzip data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"51ziWRO-_PF4","colab_type":"code","colab":{}},"source":["#0. Importing modules & defining global variables:\n","import os\n","import random\n","import numpy as np \n","import pandas as pd\n","import tensorflow as tf \n","from tensorflow import keras \n","from matplotlib import pyplot as plot \n","from sklearn.preprocessing import StandardScaler\n","print(tf.__version__)\n","\n","#categories=['coast']\n","categories=['coast','forest','insidecity','mountain','opencountry']\n","dataset,labels=[],[]\n","scaler=StandardScaler()\n","activation_fn,batchsize,max_patience_pt,max_patience_ft,max_epochs_pt,max_epochs_ft = 'sigmoid',20,10,10,50,500\n","d1,d2,d3=300,150,70\n","d4,d5=30,len(categories)\n","K,momentum,learning_rate=2,0.95,0.01\n","dir='./data/data1/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTeS9pMU_otA","colab_type":"code","colab":{}},"source":["#1. Data pre-processing & data-loading function:\n","def data():\n","    dataset,labels=[],[]\n","    for i in range(len(categories)):\n","\t    sub_dir=dir+categories[i]+'/'\n","\t    for filename in os.listdir(sub_dir):\n","\t\t    print('Reading ',filename,'...')\n","\t\t    temp=pd.read_csv(sub_dir+filename,delim_whitespace=True,header=None).to_numpy()\n","\t\t    temp=np.reshape(temp,(temp.size))\n","\t\t    dataset.append(temp)\n","\t\t    labels.append(np.array([i]))\n","    dataset=np.array(dataset,dtype=np.float32)\n","    labels=np.concatenate(labels,axis=0)\n","    combined=list(zip(dataset,labels))\n","    random.shuffle(combined)\n","    dataset,labels=zip(*combined)\n","    dataset,labels=np.array(dataset),np.array(labels)\n","    dataset=scaler.fit_transform(dataset)\n","    print('Shape of dataset & labels: ',dataset.shape,' & ',labels.shape)\n","    return dataset,labels\n","\n","dataset,labels=data()\n","train_dataset,validate_dataset,test_dataset=np.split(dataset,[int(.7*len(dataset)),int(.8*len(dataset))])\n","train_labels,validate_labels,test_labels=np.split(labels,[int(.7*len(labels)),int(.8*len(labels))])\n","print('Shape of dataset & labels (train, valid, test): (',train_dataset.shape,',',validate_dataset.shape,',',test_dataset.shape,')',' & (',train_labels.shape,',',validate_labels.shape,',',test_labels.shape,')')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsXp18VV_imh","colab_type":"code","colab":{}},"source":["#2. Classes & functions for building model:\n","\n","class Model(object):\n","\tdef __init__(self,input_shape,n=3,sigma=1):\n","\t\tself.n=n\n","\t\tself.sigma = sigma\n","\t\tself.w01 = tf.Variable(tf.initializers.GlorotUniform()(shape=(d1,input_shape)),dtype=tf.float32)\n","\t\tself.b0 = tf.Variable(tf.initializers.Zeros()(shape=input_shape),dtype=tf.float32)\n","\t\tself.x00 = tf.Variable(tf.initializers.Zeros()(shape=input_shape),dtype=tf.float32)\n","\t\tself.x0k = tf.Variable(tf.initializers.Zeros()(shape=input_shape),dtype=tf.float32)\n","\t\tself.b1 = tf.Variable(tf.initializers.Zeros()(shape=d1),dtype=tf.float32)\n","\t\tself.x10 = tf.Variable(tf.initializers.Zeros()(shape=d1),dtype=tf.float32)\n","\t\tself.x1k = tf.Variable(tf.initializers.Zeros()(shape=d1),dtype=tf.float32)\n","\t\tif n >= 2:\n","\t\t\tself.w12 = tf.Variable(tf.initializers.GlorotUniform()(shape=(d2,d1)),dtype=tf.float32)\n","\t\t\tself.b2 = tf.Variable(tf.initializers.Zeros()(shape=d2),dtype=tf.float32)\n","\t\t\tself.x20 = tf.Variable(tf.initializers.Zeros()(shape=d2),dtype=tf.float32)\n","\t\t\tself.x2k = tf.Variable(tf.initializers.Zeros()(shape=d2),dtype=tf.float32)\n","\t\tif n >= 3:\n","\t\t\tself.w23 = tf.Variable(tf.initializers.GlorotUniform()(shape=(d3,d2)),dtype=tf.float32)\n","\t\t\tself.b3 = tf.Variable(tf.initializers.Zeros()(shape=d3),dtype=tf.float32)\n","\t\t\tself.x30 = tf.Variable(tf.initializers.Zeros()(shape=d3),dtype=tf.float32)\n","\t\t\tself.x3k = tf.Variable(tf.initializers.Zeros()(shape=d3),dtype=tf.float32)\n","#\t\tself.sigma = tf.Variable(tf.initializers.Ones()(shape=input_shape),dtype=tf.float32)\n","\n","\tdef __call__(self,x0):\n","\t\tif self.n is 3:\t\t\n","\t\t\treturn self.bins(tf.linalg.matvec(self.w23,self.bins(tf.linalg.matvec(self.w12,self.bins(tf.linalg.matvec(self.w01,x0)+self.b1))+self.b2))+self.b3)\n","\t\telif self.n is 2:\n","\t\t\treturn self.bins(tf.linalg.matvec(self.w12,self.bins(tf.linalg.matvec(self.w01,x0)+self.b1))+self.b2)\n","\t\telif self.n is 1:\n","\t\t\treturn self.bins(tf.linalg.matvec(self.w01,x0)+self.b1)\n","\n","\tdef feedfrwd(self,x0,id=0):\n","\t\tif id is 1:\n","\t\t\tself.x00=x0\n","\t\t\tself.x10=tf.math.sigmoid(tf.linalg.matvec(self.w01,self.x00)+self.b1)\n","\t\t\tout_b1=self.bins(self.x10,1)\n","\t\t\tif self.n is 1:\n","\t\t\t\treturn out_b1\n","\t\t\tself.x20=tf.math.sigmoid(tf.linalg.matvec(self.w12,out_b1)+self.b2)\n","\t\t\tout_b2=self.bins(self.x20,1)\n","\t\t\tif self.n is 2:\n","\t\t\t\treturn out_b2\n","\t\t\tself.x30=tf.math.sigmoid(tf.linalg.matvec(self.w23,out_b2)+self.b3)\n","\t\t\tout_b3=self.bins(self.x30,1)\n","\t\t\treturn out_b3\n","\t\telse:\n","\t\t\tif self.n is 1:\n","\t\t\t\treturn self.bins(tf.linalg.matvec(self.w01,x0)+self.b1)\n","\t\t\telif self.n is 2:\n","\t\t\t\treturn self.bins(tf.linalg.matvec(self.w12,self.bins(tf.linalg.matvec(self.w01,x0)+self.b1))+self.b2)\n","\t\t\telse:\n","\t\t\t\treturn self.bins(tf.linalg.matvec(self.w23,self.bins(tf.linalg.matvec(self.w12,self.bins(tf.linalg.matvec(self.w01,x0)+self.b1))+self.b2))+self.b3)\n","\n","\tdef feedbkwd(self,xl,id=0):\n","\t\tif id is 1:\n","\t\t\tif self.n is 3:\n","\t\t\t\tself.x3k=self.bins(xl,1)\n","\t\t\t\tself.x2k=tf.math.sigmoid(tf.linalg.matvec(tf.transpose(self.w23),self.x3k)+self.b2)\n","\t\t\t\tout_b2=self.bins(self.x2k,1)\n","\t\t\t\tself.x1k=tf.math.sigmoid(tf.linalg.matvec(tf.transpose(self.w12),out_b2)+self.b1)\n","\t\t\t\tout_b1=self.bins(self.x1k)\n","\t\t\t\tself.x0k=self.gauss(tf.linalg.matvec(tf.transpose(self.w01),out_b1)+self.b0)\n","\t\t\telif self.n is 2:\n","\t\t\t\tself.x2k=self.bins(xl,1)\n","\t\t\t\tself.x1k=tf.math.sigmoid(tf.linalg.matvec(tf.transpose(self.w12),self.x2k)+self.b1)\n","\t\t\t\tout_b1=self.bins(self.x1k)\n","\t\t\t\tself.x0k=self.gauss(tf.linalg.matvec(tf.transpose(self.w01),out_b1)+self.b0)\n","\t\t\telif self.n is 1:\n","\t\t\t\tself.x1k=self.bins(xl,1)\n","\t\t\t\tself.x0k=self.gauss(tf.linalg.matvec(tf.transpose(self.w01),self.x1k)+self.b0)\n","\t\t\treturn self.x0k\n","\t\telse:\n","\t\t\tif self.n is 3:\n","\t\t\t\treturn self.gauss(tf.linalg.matvec(tf.transpose(self.w01),self.bins(tf.linalg.matvec(tf.transpose(self.w12),self.bins(tf.linalg.matvec(tf.transpose(self.w23),self.bins(xl,1))+self.b2))+self.b1))+self.b0)\n","\t\t\telif self.n is 2:\n","\t\t\t\treturn self.gauss(tf.linalg.matvec(tf.transpose(self.w01),self.bins(tf.linalg.matvec(tf.transpose(self.w12),self.bins(xl,1))+self.b1))+self.b0)\n","\t\t\telif self.n is 1:\n","\t\t\t\treturn self.gauss(tf.linalg.matvec(tf.transpose(self.w01),xl)+self.b0)\n","\n","\tdef gauss(self,inp):\n","\t\treturn inp+tf.random.normal(tf.shape(inp),mean=0.0,stddev=self.sigma,dtype=tf.float32)\n","\n","\tdef bins(self,inp,id=0):\n","\t\tif id is 0:\n","\t\t\treturn tf.floor(tf.math.sigmoid(inp)+tf.random.uniform(tf.shape(inp),0,1))  #Sigmoid of input & then binarying.\n","#\t\t\treturn tf.floor(tf.math.sigmoid(inp)+0.5*tf.ones(shape=inp.shape))\n","\t\telse:\n","\t\t\treturn tf.floor(inp+tf.random.uniform(tf.shape(inp),0,1))  #Binarying.\n","#\t\t\treturn tf.floor(inp+tf.repeat(0.5,inp.shape))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VAell4FHAHex","colab_type":"code","colab":{}},"source":["#3. Pre-training Restricted Boltzmann Machines:\n","def pt_rbm(dataset,K=K,n=3):\n","    print(\"Pre-training RBM's...\")\n","    input_shape=dataset.shape[-1]\n","    model = Model(input_shape,n)\n","    min_valid_loss,patience=10,0\n","    inp,out=tf.Variable(tf.initializers.Zeros()(shape=input_shape),dtype=tf.float32),tf.Variable(tf.initializers.Zeros()(shape=d3),dtype=tf.float32)\n","    for epoch in range(max_epochs_pt+1):\n","        if epoch is 0:\n","            min_valid_loss=np.square(model.feedbkwd(model(validate_dataset))-validate_dataset).mean(axis=1).mean()\n","        validate_loss=np.square(model.feedbkwd(model(validate_dataset))-validate_dataset).mean(axis=1).mean()\n","        if min_valid_loss < validate_loss:\n","            min_valid_loss=validate_loss\n","            patience=0\n","        else:\n","            patience=patience+1\n","        if patience >= int(max_patience_pt**2):\n","            break\n","        for i in range(len(dataset)):\n","            inp=dataset[i]\n","            for k in range(K):\n","                print('In ',k+1,'th step of ',epoch,'/',max_epochs_pt,' epochs in ',i,'th dataset with n=',n)\n","                if k is 0:\n","                    out=model.feedfrwd(inp,1)\n","                    inp=model.feedbkwd(out,1)\n","                elif k is K-1:\n","                    out=model.feedfrwd(inp)\n","                    inp=model.feedbkwd(out,1)\n","                else:\n","                    out=model.feedfrwd(inp)\n","                    inp=model.feedbkwd(out)\n","            dw01 = tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w01,model.x00)+model.b1),model.x00,0) - tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w01,model.x0k)+model.b1),model.x0k,0)\n","            db0 = model.x00 - model.x0k\n","            db1_0 = model.x10 - tf.math.sigmoid(tf.linalg.matvec(model.w01,model.x0k)+model.b1)\n","            db1 = db1_0 \n","            if n >= 2:            \n","                db1_2 = model.x10 - model.x1k\n","                db1 = db1 + db1_2\n","                dw12 = tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w12,model.x10)+model.b2),model.x10,0) - tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w12,model.x1k)+model.b2),model.x1k,0)\n","                db2_1 = tf.math.sigmoid(tf.linalg.matvec(model.w12,model.x10)+model.b2) - tf.math.sigmoid(tf.linalg.matvec(model.w12,model.x1k)+model.b2)\n","                db2 = db2_1 \n","            if n >= 3:\n","                db2_3 = model.x20 - model.x2k\n","                db2 = db2_3 + db2\n","                dw23 = tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w23,model.x20)+model.b3),model.x20,0) - tf.tensordot(tf.math.sigmoid(tf.linalg.matvec(model.w23,model.x2k)+model.b3),model.x2k,0)\n","                db3 = tf.math.sigmoid(tf.linalg.matvec(model.w23,model.x20)+model.b3) - tf.math.sigmoid(tf.linalg.matvec(model.w23,model.x2k)+model.b3)\n","            model.w01=model.w01 + learning_rate*dw01\t\t\n","            model.b0=model.b0 + learning_rate*db0\n","            model.b1=model.b1 + learning_rate*db1\n","            if n >= 2:\n","                model.w12=model.w12 + learning_rate*dw12\t\t\n","                model.b2=model.b2 + learning_rate*db2\n","            if n >= 3:\n","                model.w23=model.w23 + learning_rate*dw23\t\t\n","                model.b3=model.b3 + learning_rate*db3          \n","#                model.b3=model.b3*momentum + (learning_rate*(1-momentum)/tf.cast(tf.shape(db3)[0],dtype=tf.float32))*db3          \n","    return model\n","\n","#model1=pt_rbm(train_dataset,n=1)\n","#model2=pt_rbm(train_dataset,n=2)\n","model3=pt_rbm(train_dataset,n=3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCsJByfeAMab","colab_type":"code","colab":{}},"source":["#4. Fine-tuning:\n","def classifier(inp):\n","\tdense4=tf.keras.layers.Dense(d4,activation=activation_fn)\n","\tdense5=tf.keras.layers.Dense(d5,activation='softmax')\n","\treturn dense5(dense4(inp))\n","\n","def ft_dense(shape):\n","    print('Fine tuning...')\n","    inp1=keras.Input(shape=shape)\n","    out1=classifier(inp1)\n","    model=keras.models.Model(inputs=inp1,outputs=out1)\n","    model.compile(optimizer='adam',loss=keras.losses.CategoricalCrossentropy(),metrics=['mae','mse','accuracy'])\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhQojHO_DI14","colab_type":"code","colab":{}},"source":["#5. Integration:\n","\n","if __name__ == '__main__':\n","    m1=ft_dense(d1)\n","    m2=ft_dense(d2)\n","    m3=ft_dense(d3)\n","    earlystop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=int(max_patience_ft**2))\n","    history1=m1.fit(model1(train_dataset),tf.one_hot(train_labels,len(categories)),batch_size=batchsize,callbacks=[earlystop],epochs=max_epochs_ft,validation_data=(model1(validate_dataset),tf.one_hot(validate_labels,len(categories))))\n","    history2=m2.fit(model2(train_dataset),tf.one_hot(train_labels,len(categories)),batch_size=batchsize,callbacks=[earlystop],epochs=max_epochs_ft,validation_data=(model2(validate_dataset),tf.one_hot(validate_labels,len(categories))))\n","    history3=m3.fit(model3(train_dataset),tf.one_hot(train_labels,len(categories)),batch_size=batchsize,callbacks=[earlystop],epochs=max_epochs_ft,validation_data=(model3(validate_dataset),tf.one_hot(validate_labels,len(categories))))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aKZYDs4URLXK","colab_type":"code","colab":{}},"source":["model1(train_dataset).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mgBD3Z9sDPwY","colab_type":"code","colab":{}},"source":["#6. Plots of model results:\n","def plots(history):\n","    print(history.history.keys())\n","    plot.plot(history.history['accuracy'])\n","    plot.plot(history.history['val_accuracy'])\n","    plot.title('Model_Accuracy vs #Epochs')\n","    plot.ylabel('Accuracy')\n","    plot.xlabel('#Epochs')\n","    plot.legend(['Train', 'Validation'], loc='upper right')\n","    plot.show()\n","    plot.plot(history.history['loss'])\n","    plot.plot(history.history['val_loss'])\n","    plot.title('Model_Loss (CrossEntropy) vs #Epochs')\n","    plot.ylabel('Loss')\n","    plot.xlabel('#Epochs')\n","    plot.legend(['Train', 'Validation'], loc='upper right')\n","    plot.show()\n","plots(history3)\n"],"execution_count":null,"outputs":[]}]}