# -*- coding: utf-8 -*-
"""a3_3.1.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wuok7b5nizrHWtSR36gYk8tZAcPpXta_
"""

# Commented out IPython magic to ensure Python compatibility.
#-1: Setting up google colab:
from google.colab import drive
drive.mount('/content/gdrive',force_remount=True)
# %cd '/content/gdrive/My Drive/CS6910_DL/a3/'
!ls
#!unzip data.zip

#0. Importing modules & defining global variables:
import os
import random
import numpy as np 
import pandas as pd
import seaborn as sns
import tensorflow as tf 
from sklearn import metrics
from tensorflow import keras
from matplotlib import pyplot as plot 
from tensorflow.keras import backend as bk
from tensorflow.keras.preprocessing.image import *
from tensorflow.keras.preprocessing.image import ImageDataGenerator
print(tf.__version__)

dataset,labels=[],[]
h,w=250,250
dir='./data/CUB_200_2011/images/'
#categories=['003.Sooty_Albatross','018.Spotted_Catbird']
categories=['003.Sooty_Albatross','018.Spotted_Catbird','041.Scissor_tailed_Flycatcher','047.American_Goldfinch','114.Black_throated_Sparrow','123.Henslow_Sparrow','179.Tennessee_Warbler']
K,n_filters_conv1,n_filters_conv2,max_patience,batchsize,max_epochs=4,4,16,10,10,100

#1. Image pre-processing & data-loading function:
def data(h=h,w=w):
    dataset,labels=[],[]
    for i in range(len(categories)):
        j=1
        for filename in os.listdir(dir+categories[i]):
            print('Reading ',j,'.',filename,'...')
            j=j+1
            img=img_to_array(load_img(dir+categories[i]+'/'+filename,target_size=(h,w)))
            dataset.append(img)
            labels.append(i)
            sample=np.expand_dims(img,0)
            if j%2 is 0:
                dataset.append(ImageDataGenerator(width_shift_range=np.random.uniform(0,1)).flow(sample,batch_size=1).next()[0].astype('uint8'))
                labels.append(i)
                dataset.append(ImageDataGenerator(vertical_flip=np.random.randint(0,2),horizontal_flip=np.random.randint(0,2)).flow(sample,batch_size=1).next()[0].astype('uint8'))
                labels.append(i)
                dataset.append(ImageDataGenerator(brightness_range=[np.random.uniform(0,1),np.random.uniform(1,2)]).flow(sample,batch_size=1).next()[0].astype('uint8'))
                labels.append(i)    
            else:
                dataset.append(ImageDataGenerator(height_shift_range=np.random.uniform(0,1)).flow(sample,batch_size=1).next()[0].astype('uint8'))
                labels.append(i)
                dataset.append(ImageDataGenerator(rotation_range=360*np.random.uniform(0,1)).flow(sample,batch_size=1).next()[0].astype('uint8'))
                labels.append(i)
                dataset.append(ImageDataGenerator(zoom_range=[np.random.uniform(0,1),np.random.uniform(1,2)]).flow(sample,batch_size=1).next()[0].astype('uint8'))
                labels.append(i)
    combined=list(zip(dataset,labels))
    random.shuffle(combined)
    dataset,labels=zip(*combined)
    dataset,labels=np.array(dataset)/255.0,np.array(labels)
    print('Shape of dataset & labels: ',dataset.shape,' & ',labels.shape)
    return dataset, labels
dataset,labels=data(h,w)
train_dataset,validate_dataset,test_dataset=np.split(dataset,[int(.7*len(dataset)),int(.8*len(dataset))])
train_labels,validate_labels,test_labels=np.split(labels,[int(.7*len(labels)),int(.8*len(labels))])
print('Shape of dataset & labels (train, valid, test): (',train_dataset.shape,',',validate_dataset.shape,',',test_dataset.shape,')',' & (',train_labels.shape,',',validate_labels.shape,',',test_labels.shape,')')

#2. Classes & function for building model:
class NetVLADLayer(tf.keras.layers.Layer):
	def __init__(self,num_clusters,n=0,**kwargs):
		self.K = num_clusters
		self.n = n
		super(NetVLADLayer,self).__init__(**kwargs)

	def build(self,input_shape):
		self.D = input_shape[-1]
		self.kernel = self.add_weight(shape=(1,1,self.D,self.K),initializer='GlorotUniform',trainable=True,name='alpha')
		if self.n is 0:  #Unconstrained approach!
			self.bias = self.add_weight(shape=(1,1,self.K),initializer='uniform',trainable=True,name='bias')
			self.C = self.add_weight(shape=[1,1,1,self.D,self.K],initializer='GlorotUniform',trainable=True,name='beta')
		else:  #Constrained apprach where 'C' and 'bias' are dependent on 'kernel' by an expression
			self.bias = self.add_weight(shape=(1,1,self.K),initializer='uniform',trainable=False,name='bias')
			self.C = self.add_weight(shape=[1,1,1,self.D,self.K],initializer='GlorotUniform',trainable=False,name='beta')
		super(NetVLADLayer,self).build(input_shape)  

	def call(self,x):
		input_shape = tf.shape(x)
		if self.n is 1:
			self.C=self.kernel  #2nd constrained approach!
			self.bias=-0.5*tf.reduce_sum(tf.square(self.C),axis=2)  #2nd constrained approach!
		a = bk.expand_dims(bk.softmax(bk.conv2d(x,self.kernel,padding='same')+self.bias),-2)
		z_k = a * (bk.expand_dims(x,-1)+self.C)
		z = bk.sum(z_k, axis=[1, 2])
		z = bk.permute_dimensions(z, pattern=[0, 2, 1])
		z = bk.l2_normalize(z,axis=-1)
		z = bk.batch_flatten(z)
		z = bk.l2_normalize(z,axis=-1)
		return tf.reshape(z, self.compute_output_shape(input_shape))

	def compute_output_shape(self,input_shape):
		return (input_shape[0],self.K*self.D)

	def get_config(self):
		pass
		config = {'num_clusters': self.K}
		base_config = super(NetVLADLayer, self).get_config()
		return dict(list(base_config.items()) + list(config.items()))

def build_model(inp_shape,k=K):
    conv1=keras.layers.Conv2D(filters=n_filters_conv1,kernel_size=(3,3),strides=(1,1),activation='relu')
    maxp1=keras.layers.AveragePooling2D(pool_size=(2,2),strides=(2,2))
    maxp2=keras.layers.AveragePooling2D(pool_size=(2,2),strides=(2,2))
	conv2=keras.layers.Conv2D(filters=n_filters_conv2,kernel_size=(3,3),strides=(1,1),activation='relu')
    netvlad1=NetVLADLayer(k,n=0)
    flat1=keras.layers.Flatten()
    dense1=keras.layers.Dense(len(categories),activation='softmax')
    inp1=keras.Input(shape=inp_shape)
    out_conv1=conv1(inp1)
    out_maxp1=maxp1(out_conv1)
    out_conv2=conv2(out_maxp1)
    out_maxp2=maxp2(out_conv2)		
    out_netvlad1=netvlad1(out_maxp2)  #NetVLAD layer appended after MeanPool2!
    out1=dense1(out_netvlad1)
    model=keras.models.Model(inputs=inp1,outputs=out1)
    model.compile(metrics=['mae','mse','accuracy'],optimizer='adam',loss=keras.losses.CategoricalCrossentropy())  #keras.optimizers.RMSprop(0.0001)
    return model

#3. Plots of model results:
def plots(history):
    print(history.history.keys())
    plot.plot(history.history['accuracy'])
    plot.plot(history.history['val_accuracy'])
    plot.title('Model_Accuracy vs #Epochs')
    plot.ylabel('Accuracy')
    plot.xlabel('#Epochs')
    plot.legend(['Train', 'Test'], loc='upper right')
    plot.grid()
    plot.show()
    plot.plot(history.history['loss'])
    plot.plot(history.history['val_loss'])
    plot.title('Model_Loss (CrossEntropy) vs #Epochs')
    plot.ylabel('Loss')
    plot.xlabel('#Epochs')
    plot.legend(['Train', 'Test'], loc='upper right')
    plot.grid()
    plot.show()

#4. Integration:
if __name__ == '__main__':
    model=build_model(inp_shape=train_dataset.shape[1:],k=K)
    earlystop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=max_patience)
    history=model.fit(train_dataset,tf.one_hot(train_labels,len(categories)),callbacks=[earlystop],batch_size=batchsize,epochs=max_epochs*10,validation_data=(validate_dataset,tf.one_hot(validate_labels,len(categories))))
    model.save("models/model_adam_unconstrained1")
#1. Plotting of history (acc,loss etc. vs #epochs):
    plots(history)
#2. Results on test data:
    results=model.evaluate(test_dataset,tf.one_hot(test_labels,len(categories)))
#3. Confusion Matrix:
    y_preds=model.predict(test_dataset)
    y_test=test_labels
    y_preds=np.argmax(y_preds, axis=1)
    conf_mat = metrics.confusion_matrix(y_test, y_preds)
    team_csvs=['003.Sooty_Albatross','018.Spotted_Catbird','041.Scissor_tailed_Flycatcher','047.American_Goldfinch','114.Black_throated_Sparrow','123.Henslow_Sparrow','179.Tennessee_Warbler']
    sns.heatmap(conf_mat, annot=True, cmap='BuPu',xticklabels=team_csvs, yticklabels=team_csvs).set_title("Confusion Matrix")

model1=model
#model1.summary()
model1._layers.pop()
#model1.summary()
outie=model1.predict(train_dataset)
print(outie.shape)