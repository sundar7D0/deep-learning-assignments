{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"a4_1.ipynb","provenance":[],"private_outputs":true,"authorship_tag":"ABX9TyMzOosuHMr8wzCnl5WuAkZv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"lzPSmULMSxCD"},"source":["#-1: Setting up google colab:\n","from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)\n","%cd '/content/gdrive/My Drive/CS6910_DL/a4/'\n","!ls\n","#!unzip data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4hbB0p_S9TL"},"source":["#0. Importing modules & defining global variables:\n","import os\n","import csv\n","import pickle\n","import random\n","import string\n","import numpy as np \n","import pandas as pd\n","import seaborn as sns\n","import tensorflow as tf \n","from sklearn import metrics\n","from tensorflow import keras\n","from matplotlib import pyplot as plot \n","from tensorflow.keras import backend as bk\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.text import *\n","from tensorflow.keras.preprocessing.image import *\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","print(tf.__version__)\n","\n","dir='./data/data1/'\n","out_vgg16=4096\n","K,D=4,3\n","val_split,max_patience,batchsize,max_epochs=0.1,5,2,100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4UR0aRMoS-vx"},"source":["#1. Image pre-processing functions:\n","def req_images(images='Test_image.txt'):  #'image_names.txt'):\n","    with open(dir+images) as f:\n","        reader=csv.reader(f,delimiter=\"\\t\")\n","        image_filename=list(reader)\n","    image_filenames=np.concatenate(image_filename)\n","    return image_filenames\n","\n","def image_featurizer(sub_dir='Images',out_pkl='features1.pkl',j_limit=4):  #features.pkil==(4096), features2.pkl==(7,7,512) of vgg16; j_limit=10k.\n","    features,j=dict(),0\n","    model1=keras.applications.VGG16()\n","    model1._layers.pop()\n","    model1._layers.pop()\n","    model=keras.models.Model(inputs=model1.inputs, outputs=model1.layers[-3].output)\n","    model.summary()\n","    for filename in os.listdir(dir+sub_dir+'/'):\n","        j=j+1\n","        if j>j_limit:\n","            break\n","        print('Reading ',j,'.',filename,'...')\n","        img=img_to_array(load_img(dir+'Images/'+filename,target_size=(224,224)))\n","        img=img.reshape((1,img.shape[0],img.shape[1],img.shape[2]))\n","        img=keras.applications.vgg16.preprocess_input(img)\n","        features[filename]=model.predict(img,verbose=0)\n","    print('Extracted Features: ',len(features))\n","#    pickle.dump(features, open(out_pkl, 'wb'))\n","\n","def load_photo_features(dataset,filename='features2.pkl'):\n","    all_features,features,j = pickle.load(open(filename, 'rb')),dict(),0\n","    for key, value in all_features.items():\n","        if key in dataset:\n","            j=j+1\n","            print('Reading features of ',j,'.',key,'...')\n","            features[key]=value\n","#\tfeatures = {k: all_features[k] for k in dataset}\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LpNK-ltvTAzJ"},"source":["#2. Text pre-processing functions:\n","def to_vocabulary(descriptions):\n","\tvocab = set()\n","\tfor key in descriptions.keys():\n","\t\t[vocab.update(d.split()) for d in descriptions[key]]\n","\treturn vocab\n","\n","def text_extractor(image_filenames,captions='captions.txt'):\n","\tmapping,table,j = dict(),str.maketrans('', '', string.punctuation),0\n","\tprint('Total of ',len(image_filenames),' unique images to be read!')\n","\twith open(dir+captions) as f:\n","\t\treader=csv.reader(f,delimiter=\"\\t\")\n","\t\tcaptions_data=list(reader)\n","\tfor i in range(len(captions_data)):\n","\t\timage_filename=str.split(str.split(captions_data[i][0],'#')[0],'.1')[0]\n","\t\tif image_filename in image_filenames:\n","\t\t\tj=j+1\n","\t\t\tprint('Reading captions of ',j,'.',image_filename,'...')\n","\t\t\tif image_filename not in mapping.keys():\n","\t\t\t\tmapping[image_filename]=[]\n","\t\t\tdescription=captions_data[i][1].split()\n","\t\t\tdescription=[word.lower() for word in description]\n","\t\t\tdescription=[word.translate(table) for word in description]\n","\t\t\tdescription=[word for word in description if len(word)>1 and word.isalpha()]\n","\t\t\tmapping[image_filename].append('startseq ' + ' '.join(description) + ' endseq')\n","\tprint('Length of vocabulary: ',len(to_vocabulary(mapping)))\n","\treturn mapping\n","\n","def all_desc(descriptions):\n","\tall_descriptions = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_descriptions.append(d) for d in descriptions[key]]\n","\treturn all_descriptions\n","\n","def max_len(descriptions):\n","\tlines = all_desc(descriptions)\n","\treturn max(len(d.split()) for d in lines)\n"," \n","def create_tokenizer(descriptions):\n","\tlines = all_desc(descriptions)\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n","\n","def data_generator1(descriptions, photos, tokenizer, max_length, vocab_size):\n","\t# loop for ever over images\n","\twhile 1:\n","\t\tfor key, desc_list in descriptions.items():\n","\t\t\t# retrieve the photo feature\n","\t\t\tphoto = photos[key][0]\n","\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n","\t\t\tyield [[in_img, in_seq], out_word]\n","def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n","\twhile 1:\n","\t\tfor key, desc in descriptions.items():\n","\t\t\tif key in photos.keys():\n","\t\t\t\tphoto = photos[key][0]\n","\t\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc, photo, vocab_size)\n","\t\t\t\tyield [[in_img, in_seq], out_word]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QU1Y5ashTCPy"},"source":["class DataGenerator(keras.utils.Sequence):\n","    def __init__(self, descriptions, tokenizer, max_length, photos, vocab_size, batch_size=2, shuffle=True):\n","        self.descriptions=descriptions\n","        self.batch_size = batch_size\n","        self.tokenizer=tokenizer\n","        self.max_length=max_length\n","        self.photos=photos\n","        self.vocab_size=vocab_size\n","        self.list_IDs = list(self.descriptions.keys())\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","        X, y = self.__data_generation(list_IDs_temp)\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, list_IDs_temp):\n","        desc=dict()\n","        for i, ID in enumerate(list_IDs_temp):\n","            desc[ID]=self.descriptions[ID]\n","        return create_sequences(self.tokenizer,self.max_length,desc,self.photos,self.vocab_size)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HT04lnHLTDyz"},"source":["#3. Classes & functions for building model:\n","class NetVLADLayer(tf.keras.layers.Layer):\n","\tdef __init__(self,num_clusters,**kwargs):\n","\t\tself.K = num_clusters\n","\t\tsuper(NetVLADLayer,self).__init__(**kwargs)\n","\n","\tdef build(self,input_shape):\n","\t\tself.D = input_shape[-1]\n","\t\tself.kernel = self.add_weight(shape=(1,1,self.D,self.K),initializer='GlorotUniform',trainable=True,name='alpha')\n","\t\tself.bias = self.add_weight(shape=(1,1,self.K),initializer='uniform',trainable=True,name='bias')\n","\t\tself.C = self.add_weight(shape=[1,1,1,self.D,self.K],initializer='GlorotUniform',trainable=True,name='beta')\n","\t\tsuper(NetVLADLayer,self).build(input_shape)  \n","\n","\tdef call(self,x):\n","\t\tinput_shape = tf.shape(x)\n","\t\ta = bk.expand_dims(bk.softmax(bk.conv2d(x,self.kernel,strides=(1,1),padding='same')+self.bias),-2)\n","\t\tz_k = a * (bk.expand_dims(x,-1)+self.C)\n","\t\tz = bk.sum(z_k, axis=[1, 2])\n","\t\tz = bk.permute_dimensions(z, pattern=[0, 2, 1])\n","\t\tz = bk.l2_normalize(z,axis=-1)\n","\t\tz = bk.batch_flatten(z)\n","\t\tz = bk.l2_normalize(z,axis=-1)\n","\t\treturn tf.reshape(z, self.compute_output_shape(input_shape))\n","\n","\tdef compute_output_shape(self,input_shape):\n","\t\treturn (input_shape[0],self.K*self.D)\n","\n","\tdef get_config(self):\n","\t\tpass\n","\t\tconfig = {'num_clusters': self.K}\n","\t\tbase_config = super(NetVLADLayer, self).get_config()\n","\t\treturn dict(list(base_config.items()) + list(config.items()))\n","\n","def build_model(vocab_size,max_length,inp_shape,K=0,drop=0.5,d1=256,embed=256,rnn=256,lstm=256,d2=256):\n","    input1=keras.Input(shape=inp_shape)\n","    input2=keras.Input(shape=(max_length,))\n","    netvlad1=NetVLADLayer(K)\n","    drop1=keras.layers.Dropout(drop)\n","    dense1=keras.layers.Dense(d1, activation='softmax')\n","    embed1=keras.layers.Embedding(vocab_size,embed,mask_zero=True)\n","    rnn1=keras.layers.SimpleRNN(rnn) \n","    lstm1=keras.layers.LSTM(lstm)  #,return_sequences=True)\n","    dense2=keras.layers.Dense(d2,activation='relu')\n","    dense3=keras.layers.Dense(vocab_size,activation='softmax')\n","\n","    out_netvlad1=netvlad1(input1)\n","    out_dense1=dense1(out_netvlad1)\n","    out_embed1=embed1(input2)\n","    out_rnn1=rnn1(drop1(out_embed1))  #lstm1(drop1(out_embed1))\n","    out_merge1=keras.layers.concatenate([out_rnn1,out_dense1])\n","    out_dense2=dense2(out_merge1)\n","    output1=dense3(out_dense2)\n","    model=keras.models.Model(inputs=[input1,input2],outputs=output1)\n","    model.compile(loss=keras.losses.CategoricalCrossentropy(),optimizer='adam',metrics=['mae','mse','accuracy'])\n","#\tprint(model.summary())\n","#\tplot_model(model, to_file='model.png', show_shapes=True)\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNk5qGTdTFGN"},"source":["#4. Create complicated sequences:\n","\n","def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n","\timgs, txts, outs = list(), list(), list()\n","\tfor key,desc_list in descriptions.items():\n","\t\tfor desc in desc_list:\n","\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n","\t\t\tfor i in range(1, len(seq)):\n","\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n","\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","\t\t\t\timgs.append(photos[key])\n","\t\t\t\ttxts.append(in_seq)\n","\t\t\t\touts.append(out_seq)\n","\treturn np.array(imgs), np.array(txts), np.array(outs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJj05glSTGbg"},"source":["#5. Data-loading function:\n","def data():\n","#    image_featurizer()\n","    image_filenames=req_images()\n","    map=text_extractor(image_filenames)\n","    tokenizer=create_tokenizer(map)\n","    vocab_size=len(tokenizer.word_index)+1\n","    max_length=max_len(map)\n","    print('Max_length of a sentence: ',max_length)\n","    photos=load_photo_features(image_filenames)\n","    print('Total photo features available: ',len(photos))\n","    print('Loaded image features and corresponding captions in sequence successfully!')\n","    return map, tokenizer, vocab_size, max_length, photos\n","map,tokenizer,vocab_size,max_length,photos=data()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oQ_-WVITKAO"},"source":["#6. Integration:\n","from itertools import islice\n","def chunks(data, SIZE=10000):\n","    it = iter(data)\n","    for i in range(0, len(data), SIZE):\n","        yield {k:data[k] for k in islice(it, SIZE)}\n","'''\n","if __name__ == '__main__':\n","#    training_generator=DataGenerator(map, tokenizer, max_length, photos, vocab_size,batch_size=2)\n","    inp_shape=photos[list(photos.keys())[0]].shape\n","    model=tf.keras.models.load_model('./models/a4_1/full_model')\n","#    model=build_model(vocab_size,max_length,inp_shape[1:],K=K)\n","    earlystop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=max_patience)\n","#    print(len(map))\n","    rm=0\n","    for i in range(2):\n","        for item in chunks(map,1000):\n","            rm=rm+1\n","            if rm >= 3:\n","                imgs,txts,outs = create_sequences(tokenizer,max_length,item,photos,vocab_size)\n","                model.fit([np.reshape(imgs,(imgs.shape[0],7,7,512)),txts],outs,epochs=5,validation_split=0.1,callbacks=[earlystop])\n","                model.save('./models/a4_1/full_model')\n","#            model.fit([imgs,txts],outs,epochs=2)\n","    #for i in range(1):\n","    #    earlystop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=max_patience)\n","    \n","#        create_sequences(tokenizer, max_length, desc, photo, vocab_size)\n","#    generator=data_generator(map,photos,tokenizer,max_length,vocab_size)\n","#        inputs,outputs=next(generator)\n","#        print(inputs)\n","#        print(outputs)\n","#        print(inputs[1].shape,outputs.shape)\n","#    x,y,z=create_sequences(tokenizer,max_length,map,photos,vocab_size)\n","#    model.fit([x,y],z,epochs=max_epochs)\n","#    history=model.fit_generator(generator,epochs=1)\n","\n","#    imgs,txts,outs=create_sequences(tokenizer,max_length,map,photos,vocab_size)\n","#    print(imgs.shape,txts.shape,outs.shape)\n","    #model=build_model(vocab_size,max_length,out_vgg16,K=K)\n","    #earlystop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=max_patience)\n","    #model.fit([imgs,txts],outs,epochs=max_epochs,batch_size=batchsize,validation_split=val_split,callbacks=[earlystop])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4iv39-NyMMzH"},"source":["def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n"," \n","# generate a description for an image\n","def generate_desc(model, tokenizer, photo, max_length):\n","\t# seed the generation process\n","\tin_text = 'startseq'\n","\t# iterate over the whole length of the sequence\n","\tfor i in range(max_length):\n","\t\t# integer encode input sequence\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pad input\n","\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n","\t\t# predict next word\n","\t\tyhat = model.predict([photo,sequence], verbose=0)\n","\t\t# convert probability to integer\n","\t\tyhat = np.argmax(yhat)\n","\t\t# map integer to word\n","\t\tword = word_for_id(yhat, tokenizer)\n","\t\t# stop if we cannot map the word\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\t# append as input for generating the next word\n","\t\tin_text += ' ' + word\n","\t\t# stop if we predict the end of the sequence\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\treturn in_text\n","\n","# evaluate the skill of the model\n","def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n","\tactual, predicted = list(), list()\n","\t# step over the whole set\n","\tfor key, desc_list in descriptions.items():\n","\t\t# generate description\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n","\t\t# store actual and predicted\n","\t\treferences = [d.split() for d in desc_list]\n","\t\tactual.append(references)\n","\t\tpredicted.append(yhat.split())\n","\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0, 1.0, 0, 0)))\n","\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0, 0, 1.0, 0)))\n","\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0, 0, 0, 1.0))) \n","#\t\tprint('key',key)\n","#\t\tprint('predicted',predicted[-1])\n","#\t\tprint('actual',actual[-1])\n","\t# calculate BLEU score\n","#\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","#\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","#\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","#\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))) \n","from nltk.translate.bleu_score import corpus_bleu\n","model=tf.keras.models.load_model('./models/a4_1/full_model')\n","evaluate_model(model,map,photos,tokenizer,max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dCHaPDQLTMiN"},"source":["#7. Plots of model results:\n","def plots(history):\n","    print(history.history.keys())\n","    plot.plot(history.history['accuracy'])\n","    plot.plot(history.history['val_accuracy'])\n","    plot.title('Model_Accuracy vs #Epochs')\n","    plot.ylabel('Accuracy')\n","    plot.xlabel('#Epochs')\n","    plot.legend(['Train', 'Test'], loc='upper right')\n","    plot.show()\n","    plot.plot(history.history['loss'])\n","    plot.plot(history.history['val_loss'])\n","    plot.title('Model_Loss (CrossEntropy) vs #Epochs')\n","    plot.ylabel('Loss')\n","    plot.xlabel('#Epochs')\n","    plot.legend(['Train', 'Test'], loc='upper right')\n","    plot.show()\n","#plots(history)"],"execution_count":null,"outputs":[]}]}